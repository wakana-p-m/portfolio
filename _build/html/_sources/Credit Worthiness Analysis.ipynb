{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit Worthiness Analysis\n",
    "\n",
    "This project aims to build classification models that analyze credit applicants' creditworthiness. Since the costs of a false positive outweigh the benefits of a true positive by a factor of 5 (Shmueli et al., 2017), the models are to be optimized to minimize the average misclassification costs in addition to improve accuracy. The dataset used for this project contains 30 variables and 1,000 records where each credit applicant had been rated as \"good credit\" (700 cases) or \"bad credit\" (300 cases), and it was divided into training (80%) and validation (20%) sets.\n",
    "\n",
    "As seen in Table 1, five models were built as the final models: two logistic regression models with full and selected independent variables and three classification trees with default settings without pruning; with pruning using a complexity parameter; a loss matrix that penalizes false positives. Statistically significant variables (p-value < 0.05.) were selected for the regression model with fewer variables based on the performance of the full model, and a few levels in the selected variables were combined into new variables to reduce dimensions. Confusion matrices were used to evaluate models’ performance. A cutoff value optimized to maximize the classification accuracy is not adequate for the classification problems with different misclassification costs (Calabrese, 2014), which applies to this project. Therefore, the cutoff value to evaluate these regression models was set to 0.7761. The “cutpointr” function was used to identify the optimal cutoff value that minimizes the misclassification cost (Thiele, 2021). \n",
    "\n",
    "AUG values indicate models’ predictive capabilities (Muschelli, 2019), and the logistic regression full model has the highest AUG value. However, the model has an overfitting risk since it used all 30 target variables. Therefore, the logistic regression model with fewer variables might be the better model, and its AUG value is 0.738, which is the second to the highest. The Loss Matrix classification tree model performs the best in terms of the average misclassification costs; however, its model accuracy is only 0.575. Two other classification trees have high accuracy, but the average misclassification costs are also higher. The classification tree - CP has the highest accuracy among all models. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, fig.width = 12, fig.height = 9)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load packages\n",
    "library(ggplot2)\n",
    "library(reshape)\n",
    "library(lattice)\n",
    "library(caret)\n",
    "library(dplyr)\n",
    "library(scales)\n",
    "library(gridExtra)\n",
    "library(pROC)\n",
    "library(cutpointr)\n",
    "library(rpart)\n",
    "library(rpart.plot)\n",
    "\n",
    "#Display options\n",
    "options(scipen = 999)\n",
    "par(pty = \"s\") #remove the extra spacing by setting the plot type to \"square\"\n",
    "options(digits = 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import data\n",
    "credit <- read.csv(\"GermanCredit.csv\")\n",
    "credit <- credit[, -1] #drop the observation column\n",
    "\n",
    "#based on the information in the textbook, there are 6 numerical variables:\n",
    "# - DURATION\n",
    "# - AMOUNT\n",
    "# - INSTALL_RATE\n",
    "# - AGE\n",
    "# - NUM_CREDITS\n",
    "# - NUM_DEPENDENTS\n",
    "\n",
    "numeric_val = c(\"DURATION\", \"AMOUNT\", \"INSTALL_RATE\", \"AGE\", \"NUM_CREDITS\", \"NUM_DEPENDENTS\")\n",
    "\n",
    "#Convert categorical variables to factors\n",
    "credit[,-which(names(credit) %in% numeric_val)] <-  lapply(credit[,-which(names(credit) %in% numeric_val)], factor)\n",
    "str(credit)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Data Exploration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(credit)\n",
    "dim(credit)\n",
    "str(credit)\n",
    "head(credit)\n",
    "\n",
    "#check the proportion of \"RESPONSE\"\n",
    "table(credit$RESPONSE)\n",
    "#0 is bad credit, 1 is good credit\n",
    "\n",
    "#show the unique values of each variables\n",
    "data.frame(unique.values = sapply(credit, n_distinct)) \n",
    "\n",
    "#Check missing values\n",
    "data.frame(missing.val = sapply(credit, function(x) sum(length(which(is.na(x)))))) \n",
    "\n",
    "#summer statistics for the numeric variables\n",
    "summary.stats <-  data.frame(\n",
    "  mean = sapply(credit[numeric_val], mean),\n",
    "  median = sapply(credit[numeric_val], median),\n",
    "  min = sapply(credit[numeric_val], min),\n",
    "  max = sapply(credit[numeric_val], max),\n",
    "  sd = sapply(credit[numeric_val], sd),\n",
    "  sum = sapply(credit[numeric_val], sum),\n",
    "  length = sapply(credit[numeric_val], length)\n",
    ")\n",
    "summary.stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Visualize the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize the distribution of numeric variables\n",
    "duration.hist <- ggplot(data=credit)+geom_histogram(mapping = aes(DURATION), binwidth = 3)\n",
    "amount.hist <- ggplot(data=credit)+geom_histogram(mapping = aes(AMOUNT), binwidth = 500)\n",
    "install.rate.bar <- ggplot(data=credit)+geom_bar(mapping = aes(factor(INSTALL_RATE)))+xlab(\"Install Rate\")\n",
    "age.hist <- ggplot(data=credit)+geom_histogram(mapping = aes(AGE), binwidth = 5)\n",
    "num.credit.bar <- ggplot(data=credit)+geom_bar(mapping = aes(x=factor(NUM_CREDITS)))+xlab(\"Number of existing credits\")\n",
    "num.dependent.bar <- ggplot(data=credit)+geom_bar(mapping = aes(x=factor(NUM_DEPENDENTS)))+xlab(\"Numer of dependents\")\n",
    "\n",
    "#show the plots in 3x2 grid\n",
    "grid.arrange(duration.hist,amount.hist,install.rate.bar,age.hist,num.credit.bar, num.dependent.bar, ncol=2, nrow=3)\n",
    "\n",
    "\n",
    "#detect multicollinearity\n",
    "#heatmap\n",
    "cor.mat <-  round(cor(credit[numeric_val]),2) #rounded correlation matrix\n",
    "melted.cor.mat <-  melt(cor.mat)\n",
    "ggplot(melted.cor.mat, mapping = aes(x= X1, y=X2, fill = value))+\n",
    "  geom_tile()+\n",
    "  geom_text(aes(x=X1, y= X2, label = value))\n",
    "\n",
    "#from caret package\n",
    "findCorrelation(x=cor(credit[numeric_val]), cutoff = 0.6, verbose = FALSE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Data Partitioning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(42)\n",
    "train.index <- sample(c(1:dim(credit)[1]), dim(credit)[1]*0.8)\n",
    "train.df <- credit[train.index,]\n",
    "valid.df <- credit[-train.index,]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Mining\n",
    "## Logistic Regression Models\n",
    "### Full model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Full model\n",
    "logit.reg.full <- glm(RESPONSE ~ ., data=train.df, family = \"binomial\")\n",
    "summary(logit.reg.full)\n",
    "round(data.frame(summary(logit.reg.full)$coefficients, odds = exp(coef(logit.reg.full))),3)\n",
    "\n",
    "#fit the model\n",
    "logit.reg.full.pred.train <- predict(logit.reg.full, train.df, type = \"response\")\n",
    "confusionMatrix(factor(ifelse(logit.reg.full.pred.train > 0.5, 1, 0)), factor(train.df$RESPONSE), positive = \"1\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Evaluate the model performance – using default cut-off value of 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate the model performance – using default cut-off value of 0.5\n",
    "logit.reg.full.pred <- predict(logit.reg.full, valid.df, type = \"response\")\n",
    "confusionMatrix(factor(ifelse(logit.reg.full.pred > 0.5, 1, 0)), factor(valid.df$RESPONSE), positive = \"1\")\n",
    "\n",
    "#roc curve and AUC\n",
    "r <- pROC::roc(valid.df$RESPONSE, logit.reg.full.pred, plot=TRUE, main=\"ROC curve\", print.auc=TRUE,\n",
    "         legacy.axes = TRUE, ylab = \"Sensitivity (True Positive rate)\", xlab = \"1-Specificity (False positive rate)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The costs of a false positive (incorrectly classifying an applicants with a bad rating as good ) outweigh the benefits of a true positive (correctly classifying the an applicant with a good credit as good) by a factor of 5.\n",
    "\n",
    "* Cost of a false positive $500\n",
    "* Opportunity cost of False negative $100 \n",
    "\n",
    "Therefore, change the cutoff value to minimize the misclassificaiton cost.\n",
    "\n",
    "#### Buidling a custom function to calculate average misclassification costs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost <-  function(act, p){\n",
    "  weight1 = 5  #false positive weight\n",
    "  weight0 = 1  #false negative weight\n",
    "  c1 = (act==0)&(p==1) #TRUE if false positive - actual0 (bad) but predicted 1 (good)\n",
    "  c0 = (act==1)&(p==0) #TRUE if false negative - actual1 (good) but predicted 0 (bad)\n",
    "  return(mean(weight1*c1+weight0*c0))\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Identifying the optimal cutoff value that minimizes the misclassification costs using 'cutpointr' package\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp <- cutpointr(valid.df, logit.reg.full.pred, RESPONSE, method = minimize_metric, metric= misclassification_cost, cost_fp = 5, cost_fn = 1)\n",
    "summary(cp)\n",
    "plot(cp)\n",
    "plot_metric(cp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Changing the cutoff value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusionMatrix(factor(ifelse(logit.reg.full.pred > 0.7661 , 1, 0)), factor(valid.df$RESPONSE), positive = \"1\" )\n",
    "\n",
    "#Compare the average misclassification costs of the cut-off value 0.5 vs 0.7661\n",
    "#cut-off value: 0.5 (default)\n",
    "cost(valid.df$RESPONSE, (factor(ifelse(logit.reg.full.pred > 0.5, 1, 0))))\n",
    "\n",
    "#cut-off value: 0.7661\n",
    "cost(valid.df$RESPONSE, (factor(ifelse(logit.reg.full.pred > 0.7661, 1, 0))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Logstic Regression with selected variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create new variables that contains significant factors only\n",
    "credit$HISTORY.DELAY <- credit$HISTORY %in% c(\"4\") \n",
    "credit$SAV_ACCT3.4 <- credit$SAV_ACCT %in% c(\"3\",\"4\")\n",
    "credit$CHK_ACCT2.3 <- credit$CHK_ACCT %in% c(\"2\", \"3\")\n",
    "credit$PRESENT_RESIDENT2 <- credit$PRESENT_RESIDENT %in% c(\"2\")\n",
    "\n",
    "#includes significant variables only\n",
    "selected_vals <- c( \"RESPONSE\",\"HISTORY.DELAY\",\"CHK_ACCT2.3\",\"DURATION\", \"NEW_CAR\", \"EDUCATION\",\"AMOUNT\",\n",
    "                    \"INSTALL_RATE\",\"SAV_ACCT3.4\",\"PRESENT_RESIDENT2\",\"PROP_UNKN_NONE\",\"OTHER_INSTALL\", \"FOREIGN\")\n",
    "\n",
    "set.seed(42)\n",
    "train.index <- sample(c(1:dim(credit)[1]), dim(credit)[1]*0.8)\n",
    "train.df <- credit[train.index,]\n",
    "valid.df <- credit[-train.index,]\n",
    "\n",
    "logit.reg <- glm(RESPONSE ~ ., data = train.df[,selected_vals], family = \"binomial\")\n",
    "\n",
    "#fit the model\n",
    "logit.reg.pred.train <- predict(logit.reg, train.df[,selected_vals], type = \"response\")\n",
    "confusionMatrix(factor(ifelse(logit.reg.pred.train > 0.7661, 1, 0)), factor(train.df$RESPONSE), positive = \"1\" )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Evaluate the model performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate the model performance\n",
    "logit.reg.pred <- predict(logit.reg, valid.df[,selected_vals], type = \"response\")\n",
    "confusionMatrix(factor(ifelse(logit.reg.pred > 0.7661, 1, 0)), factor(valid.df$RESPONSE), positive = \"1\" )\n",
    "\n",
    "#roc curve and AUC\n",
    "r <- pROC::roc(valid.df$RESPONSE, logit.reg.pred, plot=TRUE, main=\"ROC curve\", print.auc = TRUE,\n",
    "         legacy.axes = TRUE, ylab = \"Sensitivity (True Positive rate)\", xlab = \"1-Specificity (False positive rate)\") #legacy.axes show the 1-specificity (=False positive rate)\n",
    "\n",
    "#calculate the average misclassification cost\n",
    "cost(valid.df$RESPONSE, (factor(ifelse(logit.reg.pred > 0.7661, 1, 0))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Tree\n",
    "### Base Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.base <- rpart(RESPONSE ~., data = train.df, method = \"class\") \n",
    "printcp(tree.base)\n",
    "plotcp(tree.base)\n",
    "rpart.plot(tree.base)\n",
    "\n",
    "#Make a prediction\n",
    "tree.base.pred.train <- predict(tree.base, train.df, type = \"class\") #set type to \"class\" to generate predicted class membership\n",
    "confusionMatrix(tree.base.pred.train,train.df$RESPONSE, positive = \"1\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Evaluate the model performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate the model with the validation set\n",
    "tree.base.valid <- predict(tree.base, valid.df, type = \"class\")\n",
    "confusionMatrix(tree.base.valid, valid.df$RESPONSE, positive = \"1\")\n",
    "tree.base.valid.prob <- predict(tree.base, valid.df, type = \"prob\")[,2] #set class to \"prob\" to get probability for the ROC curve\n",
    "\n",
    "r <- pROC::roc(valid.df$RESPONSE, tree.base.valid.prob, plot=TRUE, main=\"ROC curve\",print.auc=TRUE,\n",
    "         legacy.axes = TRUE, ylab = \"Sensitivity (True Positive rate)\", xlab = \"1-Specificity (False positive rate)\") #legacy.axes show the 1-specificity (=False positive rate)\n",
    "#calculate misclassification cost\n",
    "cost(valid.df$RESPONSE, tree.base.valid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Tree - Pruning using complexity Parameter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printcp(tree.base)\n",
    "plotcp(tree.base)\n",
    "\n",
    "#set cp = 0.029\n",
    "class.tree.cp <- rpart(RESPONSE ~., data = train.df, method = \"class\", control = rpart.control(cp=0.029)) \n",
    "rpart.plot(class.tree.cp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Evaluate the model performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate the model with validation set\n",
    "class.tree.cp.valid <- predict(class.tree.cp, valid.df, type = \"class\")\n",
    "confusionMatrix(class.tree.cp.valid, valid.df$RESPONSE, positive = \"1\")\n",
    "class.tree.cp.valid.prob <- predict(class.tree.cp, valid.df, type = \"prob\")[,2] \n",
    "\n",
    "r <- pROC::roc(valid.df$RESPONSE, class.tree.cp.valid.prob, plot=TRUE, main=\"ROC curve\",print.auc=TRUE,\n",
    "         legacy.axes = TRUE, ylab = \"Sensitivity (True Positive rate)\", xlab = \"1-Specificity (False positive rate)\") \n",
    "\n",
    "#calculate misclassification cost\n",
    "cost(valid.df$RESPONSE, class.tree.cp.valid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Classification Tree - Pruning using a loss matrix to penalize false positive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.matrix <-  matrix(c(0,1,5,0), nrow=2) #the loss matrix is structured with actual on the rows and prediction on the columns\n",
    "class.tree.lossmt <- rpart(RESPONSE ~., data = train.df, parms = list(loss=loss.matrix),method = \"class\", control = rpart.control(cp=0))\n",
    "rpart.plot(class.tree.lossmt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Evaluate the model performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate the model\n",
    "class.tree.lossmt.valid <- predict(class.tree.lossmt, valid.df, type = \"class\")\n",
    "confusionMatrix(class.tree.lossmt.valid, valid.df$RESPONSE, positive = \"1\")\n",
    "class.tree.lossmt.valid.prob <- predict(class.tree.lossmt, valid.df, type = \"prob\")[,2]\n",
    "\n",
    "#roc curve and AUC\n",
    "r <- pROC::roc(valid.df$RESPONSE, class.tree.lossmt.valid.prob, plot=TRUE, main=\"ROC curve\", print.auc = TRUE,\n",
    "         legacy.axes = TRUE, ylab = \"Sensitivity (True Positive rate)\", xlab = \"1-Specificity (False positive rate)\") \n",
    "\n",
    "#calculate misclassification cost\n",
    "cost(valid.df$RESPONSE, class.tree.lossmt.valid.prob)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Comparison\n",
    "\n",
    "#### Summarizing the model performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pROC::roc(valid.df$RESPONSE, logit.reg.pred, plot=TRUE, main=\"ROC curve\", print.auc = TRUE,col=\"#4daf4a\", lwd=4,\n",
    "         legacy.axes = TRUE, ylab = \"Sensitivity (True Positive rate)\", xlab = \"1-Specificity (False positive rate)\")\n",
    "\n",
    "plot.roc(valid.df$RESPONSE, class.tree.lossmt.valid.prob, col=\"#377eb8\", lwd=4, print.auc=TRUE, add=TRUE, print.auc.y=0.4)\n",
    "\n",
    "plot.roc(valid.df$RESPONSE, class.tree.cp.valid.prob, col=\"#dd1c77\", lwd=4, print.auc=TRUE, add=TRUE, print.auc.y=0.3) \n",
    "\n",
    "legend(\"bottomright\", legend=c(\"Logistic Regression - Selected variables\", \"Classification tree - Loss Matrix\",\"Classification tree - CP\"), col=c(\"#4daf4a\", \"#377eb8\", \"#dd1c77\"), lwd=4)\n",
    "\n",
    "\n",
    "models <- c(\"Logistic Regression full model\", \"Logistic Regression with selected variables\",\n",
    "            \"Classification Tree base model\", \"Classification Tree - CP\", \"Classification Tree - Loss Matrix\")\n",
    "accuracy <- c(0.645, 0.62, 0.71, 0.72, 0.575)\n",
    "avg.cost <- c(0.595, 0.62, 0.84, 0.82, 0.125)\n",
    "AUC <- c(0.748, 0.738, 0.712, 0.701, 0.731)\n",
    "\n",
    "performance_summary <- data.frame(models, accuracy, avg.cost, AUC)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### The model performance summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knitr::kable(\n",
    "performance_summary,\n",
    "caption = \"Table 1 The model performance summary\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": "",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
